{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from utils import quantize, generate_dataset, training_algo, adding_noise_model, testing\n",
    "from models_utils import MLP, Linear_noisy, Noisy_Inference\n",
    "from collections import OrderedDict\n",
    "\n",
    "# what device is the code running on?\n",
    "if torch.backends.mps.is_available(): device = torch.device('mps')\n",
    "else: \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR100 or Mini Imagenet\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 120\n",
    "max_lr = 0.001\n",
    "grad_clip = 0.01\n",
    "weight_decay =0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = generate_dataset( task='cifar10', train_batch_size=batch_size, test_batch_size=batch_size*2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter( train_loader ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the CNN model \n",
    "---\n",
    "For the moment, only mobilenet_v2 is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_mobilenet_v2 import mobilenet_v2_noise as mobilenet_v2\n",
    "#from torchvision.models.mobilenetv2 import mobilenet_v2\n",
    "#model = mobilenet_v2( weights=None ) #mobilenet_v2( weights='IMAGENET1K_V1' )\n",
    "model = mobilenet_v2( weights='IMAGENET1K_V2' )\n",
    "model.classifier[1] = torch.nn.Linear( in_features=model.classifier[1].in_features, out_features=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.models.efficientnet_b0 import efficientnet_b0\n",
    "#model = mobilenet_v2( weights=None ) #mobilenet_v2( weights='IMAGENET1K_V1' )\n",
    "from torchvision.models import efficientnet_b0\n",
    "model = efficientnet_b0( weights='IMAGENET1K_V1' )\n",
    "model.classifier = torch.nn.Linear( in_features=model.classifier[1].in_features, out_features=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet_v2 has 2236682 params\n"
     ]
    }
   ],
   "source": [
    "tot_params = 0\n",
    "for p in model.parameters():\n",
    "    tot_params += len( p.flatten() )\n",
    "print(f'mobilenet_v2 has {tot_params} params')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_algo( training_type, model, data_loaders, device='cpu', lr=1e-3, clip_w=2.5, epochs=10, epochs_noise=2, \n",
    "                   noise_sd=1e-2, noise_every=100, levels=None, num_levels=15, print_every=1, verbose=False ):\n",
    "\n",
    "    train_loader, test_loader = data_loaders\n",
    "    criterion = torch.nn.NLLLoss() #torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD( model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4 ) #torch.optim.Adam( model.parameters(), lr=lr )\n",
    "    #optimizer = torch.optim.Adam( model.parameters(), lr=lr, weight_decay=5e-4 )\n",
    "    model = model.to(device)\n",
    "\n",
    "    losses_train, accs_train = [], []\n",
    "    for e in range(epochs):\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        tot_samples = 0\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            yhat = model( x )\n",
    "            y_soft = torch.nn.functional.log_softmax( yhat, dim=1 )\n",
    "            loss = criterion( y_soft, y.long() )\n",
    "            loss.backward()\n",
    "\n",
    "            if training_type == 'qat_noise' or training_type == 'qat':\n",
    "                for p in list(model.parameters()):\n",
    "                        if hasattr(p,'hid'):\n",
    "                            p.data.copy_(p.hid)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append( loss.item() )\n",
    "            correct += torch.eq( torch.argmax(yhat, dim=1), y ).cpu().sum()\n",
    "            tot_samples += len(y)\n",
    "\n",
    "            #if e+1 > epochs - epochs_noise and batch_idx%noise_every==0 and training_type == 'noise_fine_tuning':\n",
    "            #    with torch.no_grad():\n",
    "            #        for p in model.parameters():\n",
    "            #            delta_w = torch.abs( p.max()-p.min() )\n",
    "            #            n = torch.randn_like( p )*(noise_sd*delta_w)\n",
    "            #            p.copy_( p+n )\n",
    "            \n",
    "            if clip_w is not None:\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        std_w = torch.std( p )\n",
    "                        p.clip_( -std_w*clip_w, +std_w*clip_w )\n",
    "\n",
    "            if training_type == 'qat':\n",
    "                for p in list(model.parameters()):  # updating the hid attribute\n",
    "                    if hasattr(p,'hid'):\n",
    "                        p.hid.copy_(p.data)\n",
    "                    p.data = quantize( parameters=p.data, levels=levels, num_levels=num_levels, device=device )\n",
    "\n",
    "            if training_type == 'qat_noise':\n",
    "                for p in list(model.parameters()):  # updating the hid attribute\n",
    "                    if hasattr(p,'hid'):\n",
    "                        p.hid.copy_(p.data)\n",
    "                    p.data = quantize( parameters=p.data, levels=levels, num_levels=num_levels, device=device )\n",
    "                    p.data.add_( torch.randn_like(p.data)*noise_sd )\n",
    "\n",
    "        acc_train = correct/tot_samples\n",
    "        loss_train = np.mean(losses)\n",
    "        if verbose and e%print_every==0:\n",
    "            print( f'Epoch {e}, Train accuracy {acc_train*100:.2f}% Test loss {loss_train:.4f}' )\n",
    "        accs_train.append(acc_train); losses_train.append(loss_train)\n",
    "\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    tot_samples = 0\n",
    "    model.eval()\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model( x )\n",
    "        y_soft = torch.nn.functional.log_softmax( yhat, dim=1 )\n",
    "        loss = criterion( y_soft, y.long() )\n",
    "        losses.append( loss.item() )\n",
    "        correct += torch.eq( torch.argmax(yhat, dim=1), y ).cpu().sum()\n",
    "        tot_samples += len(y)\n",
    "    acc_test = correct/tot_samples\n",
    "    loss_test = np.mean(losses)\n",
    "    if verbose: print( f'-- Test accuracy {acc_test*100:.2f}% Test loss {loss_test:.4f}' )\n",
    "\n",
    "    return model, [accs_train, losses_train], [acc_test, loss_test]\n",
    "\n",
    "\n",
    "def testing( model, test_loader, device='cpu', verbose=True ):\n",
    "    '''The function assessing the test classification accuracy of the model.\n",
    "    model: model of choice\n",
    "    test_loader: the test dataloader for the task of choice\n",
    "    verbose: if True, makes the function output the test accuracy and loss'''\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    correct, tot_samples = 0, 0\n",
    "    criterion = torch.nn.NLLLoss() #torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model( x )\n",
    "        y_soft = torch.nn.functional.log_softmax( yhat, dim=1 )\n",
    "        loss = criterion( y_soft, y.long() )\n",
    "        losses.append( loss.item() )\n",
    "        correct += torch.eq( torch.argmax(yhat, dim=1), y ).cpu().sum()\n",
    "        tot_samples += len(y)\n",
    "    acc_test = correct/tot_samples\n",
    "    loss_test = np.mean(losses)\n",
    "    if verbose: print( f'-- Test accuracy {acc_test*100:.2f}% Test loss {loss_test:.4f}' )\n",
    "\n",
    "    return acc_test, loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train accuracy 47.46% Test loss 1.4597\n",
      "-- Test accuracy 63.67% Test loss 1.0362\n"
     ]
    }
   ],
   "source": [
    "data_loaders = [train_loader, test_loader]# [trainloader, testloader]\n",
    "model_trained, [accs_train, losses_train], [acc_test, loss_test] = training_algo( training_type='normal', model=model, data_loaders=data_loaders,\n",
    "                                                                                    clip_w=None, lr=1e-2, epochs=1, epochs_noise=2, \n",
    "                                                                                    print_every=1, verbose=True, device=device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
