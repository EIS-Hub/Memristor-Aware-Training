{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from utils import quantize, generate_dataset, training_algo, adding_noise_model, testing\n",
    "from models_utils import MLP, Linear_noisy, Noisy_Inference\n",
    "from collections import OrderedDict\n",
    "\n",
    "# what device is the code running on?\n",
    "if torch.backends.mps.is_available(): device = torch.device('mps')\n",
    "else: \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the SHD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m shuffled_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange( train_size ); np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle( shuffled_idx )\n\u001b[1;32m     20\u001b[0m x_val \u001b[39m=\u001b[39m {}; x_train \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 21\u001b[0m x_val[\u001b[39m'\u001b[39m\u001b[39mtimes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray( x_train_tot[\u001b[39m'\u001b[39;49m\u001b[39mtimes\u001b[39;49m\u001b[39m'\u001b[39;49m] ) [ shuffled_idx[ \u001b[39m-\u001b[39mval_size: ] ]\n\u001b[1;32m     22\u001b[0m x_val[\u001b[39m'\u001b[39m\u001b[39munits\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray( x_train_tot[\u001b[39m'\u001b[39m\u001b[39munits\u001b[39m\u001b[39m'\u001b[39m] ) [ shuffled_idx[ \u001b[39m-\u001b[39mval_size: ] ]\n\u001b[1;32m     23\u001b[0m y_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray( y_train_tot ) [ shuffled_idx[ \u001b[39m-\u001b[39mval_size: ] ]\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/h5py/_hl/dataset.py:1046\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m numpy\u001b[39m.\u001b[39mproduct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mulonglong) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1044\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_direct(arr)\n\u001b[1;32m   1047\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/h5py/_hl/dataset.py:1007\u001b[0m, in \u001b[0;36mDataset.read_direct\u001b[0;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     dest_sel \u001b[39m=\u001b[39m sel\u001b[39m.\u001b[39mselect(dest\u001b[39m.\u001b[39mshape, dest_sel)\n\u001b[1;32m   1006\u001b[0m \u001b[39mfor\u001b[39;00m mspace \u001b[39min\u001b[39;00m dest_sel\u001b[39m.\u001b[39mbroadcast(source_sel\u001b[39m.\u001b[39marray_shape):\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid\u001b[39m.\u001b[39;49mread(mspace, fspace, dest, dxpl\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dxpl)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Here we load the Dataset ###\n",
    "cache_dir = '/Volumes/KINGSTON/Datasets/'\n",
    "cache_subdir = \"SHD\"\n",
    "\n",
    "train_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_train.h5'), 'r')\n",
    "test_file = h5py.File(os.path.join(cache_dir, cache_subdir, 'shd_test.h5'), 'r')\n",
    "\n",
    "train_size = train_file['spikes']['times'].shape[0]\n",
    "val_portion = 0.1\n",
    "val_size = np.round( val_portion*train_size ).astype(np.int32)\n",
    "\n",
    "# Train and Test definition\n",
    "x_train_tot = train_file['spikes']\n",
    "y_train_tot = train_file['labels']\n",
    "x_test = test_file['spikes']\n",
    "y_test = test_file['labels']\n",
    "\n",
    "# Validation set\n",
    "shuffled_idx = np.arange( train_size ); np.random.shuffle( shuffled_idx )\n",
    "x_val = {}; x_train = {}\n",
    "x_val['times'] = np.array( x_train_tot['times'] ) [ shuffled_idx[ -val_size: ] ]\n",
    "x_val['units'] = np.array( x_train_tot['units'] ) [ shuffled_idx[ -val_size: ] ]\n",
    "y_val = np.array( y_train_tot ) [ shuffled_idx[ -val_size: ] ]\n",
    "x_train['times'] = np.array( x_train_tot['times'] ) [ shuffled_idx[ :-val_size ] ]\n",
    "x_train['units'] = np.array( x_train_tot['units'] ) [ shuffled_idx[ :-val_size ] ]\n",
    "y_train = np.array( y_train_tot ) [ shuffled_idx[ :-val_size ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\" This generator takes a spike dataset and generates spiking network input as sparse tensors. \n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.int32)\n",
    "    number_of_batches = len(labels_)//batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "    \n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            \n",
    "            coo[0].extend(batch)\n",
    "            coo[1].extend(times)\n",
    "            coo[2].extend(units)\n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "    \n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate Gradient Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):    \n",
    "    scale = 50.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "    \n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSNN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNN_old( torch.nn.Module ):\n",
    "    def __init__( self, in_features=64, hidden_size=128, out_features=20, tau_n=40e-3, tau_s=10e-3, time_stamp=1e-3,\n",
    "                  noise_forward=False, mixed_precision=False, noise_sd=0.05, num_levels=15, device='cpu' ):\n",
    "        super(RSNN_old, self).__init__()\n",
    "        self.in_feature = in_features; self.hidden_size=hidden_size; self.out_features=out_features\n",
    "        self.tau_n = tau_n; self.tau_s = tau_s; self.time_stamp = time_stamp\n",
    "        self.alpha_n = np.exp( -time_stamp/tau_n )\n",
    "        self.alpha_s = np.exp( -time_stamp/tau_s )\n",
    "        self.noise_forward = noise_forward; self.mixed_precision = mixed_precision\n",
    "        self.noise_sd = noise_sd; self.num_levels = num_levels\n",
    "        self.device = device\n",
    "\n",
    "        # weight placeholders\n",
    "        self.w_in  = torch.nn.Parameter( torch.zeros( (hidden_size, in_features), device=device ) )\n",
    "        self.w_rec = torch.nn.Parameter( torch.zeros( (hidden_size, hidden_size), device=device ) )\n",
    "        self.w_out = torch.nn.Parameter( torch.zeros( (out_features, hidden_size), device=device ) )\n",
    "        # initialization of the weights\n",
    "        torch.nn.init.kaiming_uniform_( self.w_in  )\n",
    "        torch.nn.init.kaiming_uniform_( self.w_rec )\n",
    "        torch.nn.init.kaiming_uniform_( self.w_out )\n",
    "\n",
    "    def generate_hidden_weights( self ):\n",
    "        for p in self.parameters():\n",
    "            p.hid = p.data.clone()\n",
    "\n",
    "    def forward( self, x ):\n",
    "        x = x.to_dense().permute(1,0,2)\n",
    "        x = x.to(self.device)\n",
    "        # initialize neurons and synapses\n",
    "        batch_size, t_steps = x.size(1), x.size(0)\n",
    "        syn = torch.zeros( ( batch_size, self.hidden_size ), device=self.device )\n",
    "        mem = torch.zeros( ( batch_size, self.hidden_size ), device=self.device )\n",
    "        z   = torch.zeros( ( batch_size, self.hidden_size ), device=self.device )\n",
    "        sut = torch.zeros( ( batch_size, self.out_features), device=self.device )\n",
    "        out = torch.zeros( ( batch_size, self.out_features), device=self.device )\n",
    "        # recordings\n",
    "        spk_hist, out_hist = [], []\n",
    "        for t in range(t_steps):\n",
    "            syn = syn + torch.mm( x[t], self.w_in.T ) + torch.mm( z, self.w_rec.T )\n",
    "            syn = syn * self.alpha_s\n",
    "            z = spike_fn( mem-1.0 )\n",
    "            rst = z.detach()\n",
    "            mem = mem - rst*mem\n",
    "            #mem_hist.append(mem)\n",
    "            spk_hist.append( z )\n",
    "            mem = mem + syn\n",
    "            mem = mem * self.alpha_n\n",
    "            out = out + torch.mm( z, self.w_out.T )\n",
    "            out = out * self.alpha_s\n",
    "            #out = out + sut\n",
    "            #out = out * self.alpha_n\n",
    "            out_hist.append(out)\n",
    "        spk_hist = torch.stack(spk_hist, dim=1)\n",
    "        out_hist = torch.stack(out_hist, dim=1)\n",
    "        #out_soft = torch.nn.functional.softmax(self.out, dim=-1)\n",
    "        return out_hist, spk_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFlayer(torch.nn.Module):\n",
    "    def __init__( self, in_features, out_features, tau_n=20e-3, tau_s=5e-3, time_stamp=1e-3, recurrent=False, dropout=False, device='cpu' ):\n",
    "        super(LIFlayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.tau_n, self.tau_s = tau_n, tau_s\n",
    "        self.alpha_n = np.exp( -time_stamp/tau_n )\n",
    "        self.alpha_s = np.exp( -time_stamp/tau_s )\n",
    "        self.recurrent = recurrent\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        # initialization of the weight\n",
    "        self.W = torch.nn.Linear(self.in_features, self.out_features, bias=False)\n",
    "        if recurrent: self.R = torch.nn.Linear( self.out_features, self.out_features, bias=False )\n",
    "        if dropout!=False and dropout>0.0 and dropout<1.0: self.drop = torch.nn.Dropout( p=dropout )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        batch_size = x.size(0)\n",
    "        syn = torch.zeros( ( batch_size, self.out_features ), device=self.device )\n",
    "        mem = torch.zeros( ( batch_size, self.out_features ), device=self.device )\n",
    "        spk = torch.zeros( ( batch_size, self.out_features ), device=self.device )\n",
    "        mem_hist, spk_hist = [], []\n",
    "\n",
    "        Wx = self.W( x )\n",
    "        if self.dropout!=False: Wx = self.drop( Wx )\n",
    "        for t in range( x.size(1) ):\n",
    "            syn = self.alpha_s*syn + Wx[:,t]\n",
    "            mem = self.alpha_n*(mem-spk) + syn\n",
    "            if self.recurrent: mem = mem + self.R( spk )\n",
    "            spk = spike_fn( mem - 1.0 )\n",
    "            mem_hist.append(mem)\n",
    "            spk_hist.append(spk)\n",
    "        return torch.stack( mem_hist, dim=1 ), torch.stack( spk_hist, dim=1 )\n",
    "    \n",
    "\n",
    "class LIFreadout(torch.nn.Module):\n",
    "    def __init__( self, in_features, out_features, tau_n=20e-3, tau_s=5e-3, time_stamp=1e-3, spiking=False, dropout=False, device='cpu' ):\n",
    "        super(LIFreadout, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.tau_n, self.tau_s = tau_n, tau_s\n",
    "        self.alpha_n = np.exp( -time_stamp/tau_n )\n",
    "        self.alpha_s = np.exp( -time_stamp/tau_s )\n",
    "        self.spiking = spiking\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        # initialization of the weight\n",
    "        self.W = torch.nn.Linear(self.in_features, self.out_features, bias=False)\n",
    "        if dropout!=False and dropout>0.0 and dropout<1.0: self.drop = torch.nn.Dropout( p=dropout )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        batch_size = x.size(0)\n",
    "        #syn = torch.zeros( ( batch_size, self.hidden_size ), device=self.device )\n",
    "        mem = torch.zeros( ( batch_size, self.out_features ), device=self.device )\n",
    "        spk = torch.zeros( ( batch_size, self.out_features ), device=self.device )\n",
    "        mem_hist, spk_hist = [], []\n",
    "\n",
    "        Wx = self.W( x )\n",
    "        if self.dropout!=False: Wx = self.drop( Wx )\n",
    "        for t in range( x.size(1) ):\n",
    "            mem = self.alpha_n*(mem-spk) + Wx[:, t]\n",
    "            if self.spiking: spk = spike_fn( mem - 1.0 )\n",
    "            mem_hist.append(mem)\n",
    "            spk_hist.append(spk)\n",
    "        return torch.stack( mem_hist, dim=1 ), torch.stack( spk_hist, dim=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNN( torch.nn.Module ):\n",
    "    def __init__( self, size, recurrent, dropout, tau_n=20e-3, tau_s=5e-3, time_stamp=1e-3, device='cpu' ):\n",
    "        super( RSNN, self ).__init__()\n",
    "        self.size = size # list with N integers\n",
    "        self.recurrent = recurrent # list with N-2 integers\n",
    "        self.dropout = dropout # list with N-1 floats or False entries\n",
    "        self.tau_n, self.tau_s = tau_n, tau_s\n",
    "        self.time_stamp = time_stamp\n",
    "        self.device = device\n",
    "\n",
    "        # building the model\n",
    "        layers = []\n",
    "        for s in range(len(size)-2):\n",
    "            layers = layers + [( ('lif'+str(s)), LIFlayer( in_features=size[s], out_features=size[s+1], tau_n=tau_n, tau_s=tau_s, time_stamp=time_stamp, recurrent=recurrent[s], dropout=dropout[s] ) )]\n",
    "        layers = layers + [( ('readout'), LIFreadout( in_features=size[-2], out_features=size[-1], tau_n=tau_n, tau_s=tau_s, time_stamp=time_stamp, dropout=dropout[-1] ) )]\n",
    "        self.layers = torch.nn.ModuleDict(OrderedDict( layers ))\n",
    "        \n",
    "    def generate_hidden_weights( self ):\n",
    "        for p in self.parameters():\n",
    "            p.hid = p.data.clone()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to_dense().to(self.device)\n",
    "        for i in range( len(self.size)-2 ):\n",
    "            _, x = self.layers['lif'+str(i)](x)\n",
    "        mem_rec, spk_rec = self.layers['readout'](x)\n",
    "        return mem_rec, spk_rec\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rsnn, params, x_data, y_data, nb_epochs=10, test_every=20):\n",
    "    \n",
    "    optimizer = torch.optim.Adamax(rsnn.parameters(), lr=params['lr'], betas=(0.9,0.999))\n",
    "    log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "    \n",
    "    loss_hist = []; acc_hist = []\n",
    "    acc_hist_val = []; loss_hist_val = []\n",
    "    acc_hist_test = []; loss_hist_test = []\n",
    "    for e in range(nb_epochs):\n",
    "        rsnn.train()\n",
    "        local_loss, local_acc = [], []\n",
    "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, params['batch_size'], params['nb_steps'], params['nb_inputs'], params['max_time']):\n",
    "            #output,spks = rsnn(x_local.to_dense().permute(1,0,2))\n",
    "            output,spks = rsnn( x_local )\n",
    "            m,_=torch.max(output,1) # Max Over Time of the membrane voltage\n",
    "            #m = torch.sum( output, 1 ) # Sum of the membrane voltage over time\n",
    "            am=torch.argmax(m,1)      # argmax over output units\n",
    "            #print( x_local.size(), output.size(), y_local.size() )\n",
    "            #break\n",
    "            acc_tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "            local_acc.append(acc_tmp)\n",
    "            \n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            \n",
    "            # Here we set up our regularizer loss\n",
    "            reg_loss = params['L1_total_spikes']*torch.sum(spks) # L1 loss on total number of spikes\n",
    "            reg_loss += params['L2_per_neuron']*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
    "            \n",
    "            # Here we combine supervised loss and the regularizer\n",
    "            loss_val = loss_fn(log_p_y, y_local.type(dtype=torch.long)) + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "        \n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "        mean_acc = np.mean(local_acc)\n",
    "        acc_hist.append(mean_acc)\n",
    "        #live_plot(loss_hist)\n",
    "        acc_val, loss_val = compute_classification_accuracy(rsnn, params, x_val, y_val, flag_loss=True)\n",
    "        acc_hist_val.append(acc_val); loss_hist_val.append(loss_val)\n",
    "        if (e+1)%test_every == 0:\n",
    "            acc_test, loss_test = compute_classification_accuracy(rsnn, params, x_test, y_test, flag_loss=True)\n",
    "            acc_hist_test.append(acc_test); loss_hist_test.append(loss_test)\n",
    "        print(\"Epoch %i: Train Loss=%.4f, Train Acc=%.4f; Validation Loss=%.4f, Validation Acc=%.4f\"%(e+1,mean_loss,mean_acc,loss_val,acc_val))\n",
    "        \n",
    "    return [loss_hist, acc_hist], [loss_hist_val, acc_hist_val], [loss_hist_test, acc_hist_test]\n",
    "        \n",
    "        \n",
    "def compute_classification_accuracy(rsnn, params, x_data, y_data, flag_loss=False):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "    local_loss = []\n",
    "    accs = []\n",
    "    rsnn.eval()\n",
    "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, params['batch_size'], params['nb_steps'], params['nb_inputs'], params['max_time'], shuffle=False):\n",
    "        #output,_ = rsnn(x_local.to_dense().permute(1,0,2))\n",
    "        output,_ = rsnn(x_local)\n",
    "        m,_=torch.max(output,1) # Max Over Time of the membrane voltage\n",
    "        #m = torch.sum( output, 1 ) # CumSum of the membrane voltage\n",
    "        am=torch.argmax(m,1)      # argmax over output units\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "        if flag_loss:\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "            loss_val = loss_fn(log_p_y, y_local.type(dtype=torch.long))\n",
    "            local_loss.append( loss_val.item() )\n",
    "    if flag_loss:\n",
    "        return np.mean(accs), np.mean(local_loss)\n",
    "    else:\n",
    "        return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coarse network structure and the time steps are dicated by the SHD dataset. \n",
    "\n",
    "params = {\n",
    "    'nb_inputs'  : 256,\n",
    "    'nb_hidden'  : 128,\n",
    "    'nb_outputs' : 20,\n",
    "    \n",
    "    'tau_mem' : 40e-3,\n",
    "    'tau_syn' : 5e-3,\n",
    "\n",
    "    'time_step' : 1e-3,\n",
    "    'nb_steps' : 100,\n",
    "    'max_time' : 1.4,\n",
    "    'batch_size' : 64,\n",
    "    'surrogate_grad_scale' : 50,\n",
    "    'weight_scale' : 0.2,\n",
    "    'lr' : 1e-3,\n",
    "    \n",
    "    'tech_flag' : True,\n",
    "    \n",
    "    'L1_total_spikes' : 1e-7,\n",
    "    'L2_per_neuron' : 1e-7,\n",
    "    \n",
    "    'dtype' : torch.float32,\n",
    "    'device' : device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 200\n",
    "test_every = 5\n",
    "\n",
    "size = [params['nb_inputs'], params['nb_hidden'], params['nb_outputs'] ]\n",
    "recurrent = [True]\n",
    "dropout = [False, False]\n",
    "rsnn = RSNN( size=size, recurrent=recurrent, dropout=dropout )\n",
    "\n",
    "train_stats, val_stats, test_stats = train(rsnn, params, x_train, y_train, nb_epochs=nb_epochs, test_every=test_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 100\n",
    "test_every = 5\n",
    "\n",
    "rsnn = RSNN_old(in_features=params['nb_inputs'], hidden_size=params['nb_hidden'], out_features=params['nb_outputs'], \n",
    "            tau_n=params['tau_mem'], tau_s=params['tau_syn'])\n",
    "\n",
    "train_stats, val_stats, test_stats = train(rsnn, params, x_train, y_train, nb_epochs=nb_epochs, test_every=test_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24776785714285715,\n",
       " 0.3142857142857143,\n",
       " 0.2941964285714286,\n",
       " 0.3674107142857143,\n",
       " 0.37901785714285713,\n",
       " 0.44107142857142856,\n",
       " 0.40133928571428573,\n",
       " 0.41785714285714287,\n",
       " 0.39955357142857145,\n",
       " 0.47276785714285713]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3797d9c754f161f25276ceed714d65c3ba81401549e135518acb0d24657c3be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
